# Data Preparation

## DataFrame Metadata

Column metadata is one of the most useful and the least known features of the Spark `Dataset`. It is worth noting that all features described below, although not private, are part of the developer API and as such [can be unstable](https://github.com/apache/spark/blob/v2.0.0/common/tags/src/main/java/org/apache/spark/annotation/DeveloperApi.java#L23) or even [removed in minor versions](https://github.com/apache/spark/blob/v2.0.0/common/tags/src/main/java/org/apache/spark/annotation/DeveloperApi.java#L25).

### Metadata in ML pipelines

Although it is widely used by [ML `Pipelines`](https://spark.apache.org/docs/latest/ml-pipeline.html) to indicate variable types and levels a whole process is usually completely transparent and at least partially hidden from the final user so let's look at a simple pipeline and see what happens behind the scenes.

We'll start with a simple dataset:


```scala
val df = Seq(
  (0.0, "x", 2.0),
  (1.0, "y", 3.0),
  (2.0, "x", -1.0)
).toDF("label", "x1", "x2")
```

and a following pipeline:

```scala
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler}

val stages = Array(
  new StringIndexer().setInputCol("x1").setOutputCol("x1_"),
  new VectorAssembler().setInputCols(Array("x1_", "x2")).setOutputCol("features")
)

val model = new Pipeline().setStages(stages).fit(df)
```

Now we can extract `stages`, `transform` data step-by-step:

```scala
val dfs = model.stages.scanLeft(df)((df, stage) => stage.transform(df))
```

and see what is going on at each stage:

1. Our initial dataset has no metadata:

  ```scala
  dfs(0).schema.map(_.metadata)

  // Seq[org.apache.spark.sql.types.Metadata] = List({}, {}, {}, {})
  ```

2. After transforming with `StringIndexerModel` we can see indexer specific metadata:

  ```scala
  dfs(1).schema.last.metadata

  // org.apache.spark.sql.types.Metadata =
  //   {"ml_attr":{"vals":["x","y"],"type":"nominal","name":"x1_"}}
  ```
  It is important to note that this information is stored locally so it is better to keep that in mind if number of unique values is large.

3. Finally metadata for assembled feature vector:

  ```scala
  dfs(2).schema.last.metadata

  // org.apache.spark.sql.types.Metadata = {"ml_attr":{"attrs":{
  //   "numeric":[{"idx":1,"name":"x2"}],
  //   "nominal":[{"vals":["x","y"],"idx":0,"name":"x1_"}]
  // },"num_attrs":2}}
  ```
  Metadata from upstream stages is picked by the assembler and used to describe vector indices.


Let's check if metadata is actually used in practice:

```scala
import org.apache.spark.ml.classification.DecisionTreeClassifier

new DecisionTreeClassifier().setLabelCol("label").fit(dfs.last).toDebugString

// String =
// "DecisionTreeClassificationModel (uid=dtc_72c1c370aa00) of depth 2 with 5 nodes
//   If (feature 0 in {0.0})
//    If (feature 1 <= -1.0)
//     Predict: 2.0
//    Else (feature 1 > -1.0)
//     Predict: 1.0
//   Else (feature 0 not in {0.0})
//    Predict: 0.0
// "
```

_**Note**: Prior to Spark 2.0.0 `label` column would require indexing._

As you can see nominal and numerical features are recognized and used in different ways. Which is exactly the thing we would expect.

#### Setting ML attributes manually

##### Scala

So far so good but what if you work with data which has been already preprocessed? In case like this Spark provides a set of utilities designed to create ML compliant metadata. Let's get familiar with a whole process by building metadata equivalent to the one generated by the ML pipeline we used before.

We'll need a [`NominalAttribute`](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.attribute.NominalAttribute):

```scala
import org.apache.spark.ml.attribute.NominalAttribute

val firstAttr = NominalAttribute.defaultAttr.withValues("x", "y").withName("x1_")
```
and a [`NumericAttribute`](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.attribute.NumericAttribute)

```scala
import org.apache.spark.ml.attribute.NumericAttribute

val secondAttr = NumericAttribute.defaultAttr.withName("x2")
```
Numeric attributes provide also a number of methods which can be used to store basic descriptive statistics like mean, standard deviation, minimum or maximum

Finally we combine attributes using [`AttributeGroup`](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.attribute.AttributeGroup) and convert it to [`Metadata`](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.types.Metadata) object:

```scala
import org.apache.spark.ml.attribute.AttributeGroup

val featuresMetadata = new AttributeGroup("features", Array(firstAttr, secondAttr))
```
All what is left is quick sanity check:

```scala
featuresMetadata == dfs.last.schema.last.metadata

// Boolean = true
```

Generated metadata can be applied using `Column.as` method:

```scala
/* Note We use local MLib API only to show VectorUDT usage.
 */

import org.apache.spark.mllib.linalg.Vectors

val records = Seq(
  (0.0, Vectors.dense(Array(0.0, 2.0))),
  (1.0, Vectors.dense(Array(1.0, 3.0))),
  (2.0, Vectors.dense(Array(0.0, -1.0)))
)

records.toDF("label", "features")
  .withColumn("features", $"features".as("features", featuresMetadata))
```
or added to schema when creating `DataFrame`:

```scala
import org.apache.spark.sql.types._
import org.apache.spark.mllib.linalg.VectorUDT

val schema = StructType(Seq(
  StructField("label", DoubleType, false),
  StructField("features", new VectorUDT(), false, featuresMetadata)
))
```

##### Python

Unlike Scala Python doesn't provide any helpers and metadata is simply represented as a standard Python `dict`.

```python
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, VectorAssembler

df = sc.parallelize((
    (0.0, "x", 2.0),
    (1.0, "y", 3.0),
    (2.0, "x", -1.0)
)).toDF(["label", "x1", "x2"])


model = Pipeline(stages=[
    StringIndexer(inputCol="x1", outputCol="x1_"),
    VectorAssembler(inputCols=["x1_", "x2"], outputCol="features"),
]).fit(df)

model.transform(df).schema[-1].metadata

## {'ml_attr': {'attrs': {'nominal': [{'idx': 0,
##      'name': 'x1_',
##      'vals': ['x', 'y']}],
##    'numeric': [{'idx': 1, 'name': 'x2'}]},
##   'num_attrs': 2}}
```

As for now it is not possible to attach metadata to a single column.

### Setting custom column metadata


